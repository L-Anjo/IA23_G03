{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 02 - Inteligência Artificial - LESI-PL\n",
    "### Machine Learning (ML) approaches and methods\n",
    "\n",
    "## Introdução\n",
    "O objetivo do projeto é implementar métodos de Machine Learning (ML) para problemas, utilizando datasets públicos.\n",
    "\n",
    "##### Equipa do projeto - Grupo 03\n",
    "* Pedro Martins Nº23527\n",
    "* Luís Anjo Nº23528\n",
    "* Diogo Silva Nº23893\n",
    "\n",
    "## Objetivo\n",
    "O objetivo do código trata-se da implementação de métodos e técnicas sobre dois datasets públicos. Utilizamos um dataset com informações sobre animais do jardim zoológico para implementar árvores de decisão, Naive Bayes, KNN e o clustering através do K-Means, e um outro dataset de compras de um mercado para implementar as regras de Aprendizagem e Associação.\n",
    "\n",
    "## Datasets\n",
    "**Dataset 01 - Zoo Animal Classification**\n",
    "\n",
    "Este dataset consiste em dois arquivos, o arquivo \"zoo.csv\", que contém os dados de cada animal do zoo e o arqurivo \"zooClass.csv\" que contém os detalhes das classes destes animais.\n",
    "\n",
    "Link: https://www.kaggle.com/datasets/uciml/zoo-animal-classification\n",
    "\n",
    "\n",
    "**Dataset 02 - Groceries dataset**\n",
    "\n",
    "Este dataset consiste num arquivo \"groceries.csv\", que contém informações sobre várias compras de um mercado.\n",
    "\n",
    "Link: https://www.kaggle.com/datasets/heeraldedhia/groceries-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação Automática\n",
    "\n",
    "## Import das livrarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obter o conteúdo do dataset dos Animais do Zoo e Preparação de dados\n",
    "\n",
    "Aqui serão importados os ficheiros \"zoo.csv\" e \"zooClass.csv\" do dataset.\n",
    "De forma a preparar os dados para a posterior análise, foi efetuado um \"merge\" de forma a juntar o conteúdo dos dois arquivos csv para uma só tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from csv file\n",
    "zoo_file = pd.read_csv('./datasets/zoo.csv')\n",
    "zooClass_file = pd.read_csv('./datasets/zooClass.csv')\n",
    "zoo = pd.DataFrame(zoo_file)\n",
    "zooClass = pd.DataFrame(zooClass_file)\n",
    "\n",
    "# Merge \n",
    "zoo_merge = pd.merge(zoo_file, zooClass_file, how='left', left_on='class_type', right_on='Class_Number')\n",
    "\n",
    "# Prepare the data\n",
    "zoo_feature_names = [col for col in zoo_merge.columns if col != 'Class_Number' and col != 'Number_Of_Animal_Species_In_Class' and col != 'Animal_Names' and col != 'Class_Type' and col != 'class_type' and col != 'animal_name']\n",
    "zoo_data = zoo[zoo_feature_names]\n",
    "zoo_target = zoo_merge[zoo_merge.columns.values.tolist()[20]]\n",
    "zoo_target_names = zoo_target.unique().tolist()\n",
    "zoo_target_names = list(zooClass['Class_Type'])\n",
    "\n",
    "print('Features:',zoo_feature_names, 'Classes: ', zoo_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contagem de Classes\n",
    "\n",
    "Utilizamos este gráfico para obter a distribuição dos animais presentes no dataset por cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(zooClass_file[\"Class_Type\"], zooClass_file[\"Number_Of_Animal_Species_In_Class\"])\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Class')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificação de Nulls\n",
    "\n",
    "Esta parte do código é utilizada para verificar a existência de valores Null na tabela do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Dataset\n",
    "data = \"\"\"animal_name,hair,feathers,eggs,milk,airborne,aquatic,predator,toothed,backbone,breathes,venomous,fins,legs,tail,domestic,catsize,class_type\n",
    "aardvark,1,0,0,1,0,0,1,1,1,1,0,0,4,0,0,1,1\n",
    "antelope,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
    "bass,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
    "bear,1,0,0,1,0,0,1,1,1,1,0,0,4,0,0,1,1\n",
    "boar,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "buffalo,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
    "calf,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
    "carp,0,0,1,0,0,1,0,1,1,0,0,1,0,1,1,0,4\n",
    "catfish,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
    "cavy,1,0,0,1,0,0,0,1,1,1,0,0,4,0,1,0,1\n",
    "cheetah,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "chicken,0,1,1,0,1,0,0,0,1,1,0,0,2,1,1,0,2\n",
    "chub,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
    "clam,0,0,1,0,0,0,1,0,0,0,0,0,0,0,0,0,7\n",
    "crab,0,0,1,0,0,1,1,0,0,0,0,0,4,0,0,0,7\n",
    "crayfish,0,0,1,0,0,1,1,0,0,0,0,0,6,0,0,0,7\n",
    "crow,0,1,1,0,1,0,1,0,1,1,0,0,2,1,0,0,2\n",
    "deer,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
    "dogfish,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,1,4\n",
    "dolphin,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,1\n",
    "dove,0,1,1,0,1,0,0,0,1,1,0,0,2,1,1,0,2\n",
    "duck,0,1,1,0,1,1,0,0,1,1,0,0,2,1,0,0,2\n",
    "elephant,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
    "flamingo,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,1,2\n",
    "flea,0,0,1,0,0,0,0,0,0,1,0,0,6,0,0,0,6\n",
    "frog,0,0,1,0,0,1,1,1,1,1,0,0,4,0,0,0,5\n",
    "frog,0,0,1,0,0,1,1,1,1,1,1,0,4,0,0,0,5\n",
    "fruitbat,1,0,0,1,1,0,0,1,1,1,0,0,2,1,0,0,1\n",
    "giraffe,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
    "girl,1,0,0,1,0,0,1,1,1,1,0,0,2,0,1,1,1\n",
    "gnat,0,0,1,0,1,0,0,0,0,1,0,0,6,0,0,0,6\n",
    "goat,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
    "gorilla,1,0,0,1,0,0,0,1,1,1,0,0,2,0,0,1,1\n",
    "gull,0,1,1,0,1,1,1,0,1,1,0,0,2,1,0,0,2\n",
    "haddock,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,4\n",
    "hamster,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,0,1\n",
    "hare,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,0,1\n",
    "hawk,0,1,1,0,1,0,1,0,1,1,0,0,2,1,0,0,2\n",
    "herring,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
    "honeybee,1,0,1,0,1,0,0,0,0,1,1,0,6,0,1,0,6\n",
    "housefly,1,0,1,0,1,0,0,0,0,1,0,0,6,0,0,0,6\n",
    "kiwi,0,1,1,0,0,0,1,0,1,1,0,0,2,1,0,0,2\n",
    "ladybird,0,0,1,0,1,0,1,0,0,1,0,0,6,0,0,0,6\n",
    "lark,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
    "leopard,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "lion,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "lobster,0,0,1,0,0,1,1,0,0,0,0,0,6,0,0,0,7\n",
    "lynx,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "mink,1,0,0,1,0,1,1,1,1,1,0,0,4,1,0,1,1\n",
    "mole,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,0,1\n",
    "mongoose,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "moth,1,0,1,0,1,0,0,0,0,1,0,0,6,0,0,0,6\n",
    "newt,0,0,1,0,0,1,1,1,1,1,0,0,4,1,0,0,5\n",
    "octopus,0,0,1,0,0,1,1,0,0,0,0,0,8,0,0,1,7\n",
    "opossum,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,0,1\n",
    "oryx,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,1,1\n",
    "ostrich,0,1,1,0,0,0,0,0,1,1,0,0,2,1,0,1,2\n",
    "parakeet,0,1,1,0,1,0,0,0,1,1,0,0,2,1,1,0,2\n",
    "penguin,0,1,1,0,0,1,1,0,1,1,0,0,2,1,0,1,2\n",
    "pheasant,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
    "pike,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,1,4\n",
    "piranha,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,0,4\n",
    "pitviper,0,0,1,0,0,0,1,1,1,1,1,0,0,1,0,0,3\n",
    "platypus,1,0,1,1,0,1,1,0,1,1,0,0,4,1,0,1,1\n",
    "polecat,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "pony,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
    "porpoise,0,0,0,1,0,1,1,1,1,1,0,1,0,1,0,1,1\n",
    "puma,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "pussycat,1,0,0,1,0,0,1,1,1,1,0,0,4,1,1,1,1\n",
    "raccoon,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "reindeer,1,0,0,1,0,0,0,1,1,1,0,0,4,1,1,1,1\n",
    "rhea,0,1,1,0,0,0,1,0,1,1,0,0,2,1,0,1,2\n",
    "scorpion,0,0,0,0,0,0,1,0,0,1,1,0,8,1,0,0,7\n",
    "seahorse,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,4\n",
    "seal,1,0,0,1,0,1,1,1,1,1,0,1,0,0,0,1,1\n",
    "sealion,1,0,0,1,0,1,1,1,1,1,0,1,2,1,0,1,1\n",
    "seasnake,0,0,0,0,0,1,1,1,1,0,1,0,0,1,0,0,3\n",
    "seawasp,0,0,1,0,0,1,1,0,0,0,1,0,0,0,0,0,7\n",
    "skimmer,0,1,1,0,1,1,1,0,1,1,0,0,2,1,0,0,2\n",
    "skua,0,1,1,0,1,1,1,0,1,1,0,0,2,1,0,0,2\n",
    "slowworm,0,0,1,0,0,0,1,1,1,1,0,0,0,1,0,0,3\n",
    "slug,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,7\n",
    "sole,0,0,1,0,0,1,0,1,1,0,0,1,0,1,0,0,4\n",
    "sparrow,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
    "squirrel,1,0,0,1,0,0,0,1,1,1,0,0,2,1,0,0,1\n",
    "starfish,0,0,1,0,0,1,1,0,0,0,0,0,5,0,0,0,7\n",
    "stingray,0,0,1,0,0,1,1,1,1,0,1,1,0,1,0,1,4\n",
    "swan,0,1,1,0,1,1,0,0,1,1,0,0,2,1,0,1,2\n",
    "termite,0,0,1,0,0,0,0,0,0,1,0,0,6,0,0,0,6\n",
    "toad,0,0,1,0,0,1,0,1,1,1,0,0,4,0,0,0,5\n",
    "tortoise,0,0,1,0,0,0,0,0,1,1,0,0,4,1,0,1,3\n",
    "tuatara,0,0,1,0,0,0,1,1,1,1,0,0,4,1,0,0,3\n",
    "tuna,0,0,1,0,0,1,1,1,1,0,0,1,0,1,0,1,4\n",
    "vampire,1,0,0,1,1,0,0,1,1,1,0,0,2,1,0,0,1\n",
    "vole,1,0,0,1,0,0,0,1,1,1,0,0,4,1,0,0,1\n",
    "vulture,0,1,1,0,1,0,1,0,1,1,0,0,2,1,0,1,2\n",
    "wallaby,1,0,0,1,0,0,0,1,1,1,0,0,2,1,0,1,1\n",
    "wasp,1,0,1,0,1,0,0,0,0,1,1,0,6,0,0,0,6\n",
    "wolf,1,0,0,1,0,0,1,1,1,1,0,0,4,1,0,1,1\n",
    "worm,0,0,1,0,0,0,0,0,0,1,0,0,0,0,0,0,7\n",
    "wren,0,1,1,0,1,0,0,0,1,1,0,0,2,1,0,0,2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Cria DataFrame\n",
    "df = pd.read_csv(StringIO(data), header=None)\n",
    "\n",
    "# Encontra linhas com NaN\n",
    "nan_rows = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Imprime linhas com NaN\n",
    "print(nan_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção do Modelo e Avaliação\n",
    "\n",
    "Aqui é construído o modelo de classificação, através de Árvores de decisão, utilizando a classe \"DecisionTreeClassifier\", e obtemos a tabela com os dados como resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# Data selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "zoo_target_class = zoo_merge['class_type'] # Número da classe\n",
    "X_train, X_test, y_train, y_test = train_test_split(zoo_data, zoo_target_class, test_size=0.15, random_state=27)\n",
    "\n",
    "# Train the model on the training set\n",
    "clf_model = clf.fit(X_train, y_train) # X_train: dados de (hair, feather, eggs, ...)\n",
    "                                      # y_train: número das classes (1 6 4 ...)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "hd_score = clf_model.score(X_test, y_test)\n",
    "print(\"Houldout test accuracy:\", hd_score)\n",
    "\n",
    "# Evaluate the model using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "acc_score = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "print(\"CV Mean Accuracy: %0.3f (+/- %0.3f)\" % (acc_score.mean(), acc_score.std()) )\n",
    "f1_score = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1_macro')\n",
    "print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )\n",
    "\n",
    "# More metrics: Precision Recall scores and Confusion matrix\n",
    "from sklearn import metrics\n",
    "print(\"Precision, Recall, Confusion matrix, in training test\\n\")\n",
    "print(metrics.classification_report(y_test, clf_model.predict(X_test), digits=3))\n",
    "print(metrics.confusion_matrix(y_test, clf_model.predict(X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através do método Naive Bayes, utilizando a classe \"GaussianNB\" obtemos a tabela com os dados como resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()  # Naive Bayes\n",
    "\n",
    "clf.fit(X_train, y_train) # X_train: dados de (hair, feather, eggs, ...)\n",
    "                          # y_train: número das classes (1 6 4 ...)\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"Precision, Recall, Confusion matrix, in training test\\n\")\n",
    "print(metrics.classification_report(y_test, clf.predict(X_test), digits=3))\n",
    "print(metrics.confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Através do método KNN, utilizando a classe \"KNeighborsClassifier\" obtemos a tabela com os dados como resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5) #KNN\n",
    "\n",
    "clf.fit(X_train, y_train) # X_train: dados de (hair, feather, eggs, ...)\n",
    "                          # y_train: número das classes (1 6 4 ...)\n",
    "\n",
    "# summarize the fit of the model\n",
    "print(\"Precision, Recall, Confusion matrix, in training test\\n\")\n",
    "print(metrics.classification_report(y_test, clf.predict(X_test), digits=3))\n",
    "print(metrics.confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Tree\n",
    "\n",
    "Utilizamos a função \"plot_tree\" para gerar a árvore através dos resultados obtidos anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(zoo_feature_names)\n",
    "print(zoo_target_names)\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(clf_model, \n",
    "                   feature_names=zoo_feature_names,  \n",
    "                   class_names=zoo_target_names,\n",
    "                   filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering através de K-Means\n",
    "\n",
    "## Preparação de dados\n",
    "\n",
    "Obtemos o conteúdo dos arquivos do dataset e preparamos a informação para utilizar o método K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets library\n",
    "zoo_file = pd.read_csv('./datasets/zoo.csv')\n",
    "zooClass_file = pd.read_csv('./datasets/zooClass.csv')\n",
    "zoo = pd.DataFrame(zoo_file)\n",
    "zooClass = pd.DataFrame(zooClass_file)\n",
    "\n",
    "# Prepare the data\n",
    "zoo_feature_names = [col for col in zoo.columns if col != 'animal_name' and col != 'class_type']\n",
    "zoo_data = zoo[zoo_feature_names]\n",
    "zoo_feature_names2 = [col for col in zoo.columns if col != 'animal_name']\n",
    "zoo_data2 = zoo[zoo_feature_names2[16]]-1 \n",
    "\n",
    "# Preliminary data exploration\n",
    "print(zoo_data.shape)     # Output: (101, 16)\n",
    "print(zoo_feature_names)  # Output: ['hair', 'feathers', ...]\n",
    "\n",
    "# Preprocessing the data (standardize)\n",
    "scaler = StandardScaler()\n",
    "scaled_zoo = scaler.fit_transform(zoo_data)\n",
    "print(scaled_zoo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encontrar o número ideal de clusters através do método Elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the within-cluster sum of square across different cluster counts\n",
    "inertia = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(scaled_zoo)\n",
    "    inertia.append(kmeans.inertia_)# Plot the elbow graph\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Within-cluster Sum of Square')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação e Clustering\n",
    "\n",
    "Com base nos resultados obtidos definimos um número de 5 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 clusters\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "clusters = kmeans.fit_predict(scaled_zoo)\n",
    "\n",
    "# Adjusting clusters\n",
    "adj_clusters = [(x + 2) %5 for x in clusters]\n",
    "\n",
    "print(confusion_matrix(zoo_data2.values, adj_clusters))\n",
    "print(classification_report(zoo_data2.values, adj_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização dos Clusters\n",
    "\n",
    "No gráfico seguinte será possível visualizar os clusters obtidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Animal classes and legs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(zoo_target.values,scaled_zoo[:,12], c=adj_clusters, cmap='viridis', marker='o')\n",
    "plt.title('Visualization of clustered data', fontsize=14)\n",
    "plt.xlabel('Classes', fontsize=12)\n",
    "plt.ylabel('Legs', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integração de Cluster Labels no Dataset Zoo\n",
    "\n",
    "Aqui é adicionada uma nova coluna \"clusters\" na tabela "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the zoo dataset to a DataFrame\n",
    "zoo_df = pd.DataFrame(zoo_data, columns=zoo_feature_names)\n",
    "\n",
    "# Adding the cluster labels as a new column to the DataFrame\n",
    "zoo_df['cluster'] = adj_clusters\n",
    "\n",
    "# Printing the first 5 instances of the new dataset\n",
    "print(zoo_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização 3D dos Clusters\n",
    "\n",
    "No gráfico seguinte podemos visualizar a distribuição dos clusters em 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D figure\n",
    "zoo_data3D = zoo_merge[zoo_merge.columns.values.tolist()[17]]\n",
    "\n",
    "fig = plt.figure(figsize=(16, 15))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(zoo_data3D.values,  # Class number\n",
    "           scaled_zoo[:,13],  # Tail\n",
    "           scaled_zoo[:,12],  # Legs\n",
    "           c=adj_clusters,           \n",
    "           cmap='viridis', \n",
    "           marker='o')\n",
    "\n",
    "# Setting labels\n",
    "ax.set_xlabel('Classes')\n",
    "ax.set_ylabel('Tail')\n",
    "ax.set_zlabel('Legs')\n",
    "\n",
    "# Title of the plot\n",
    "ax.set_title('3D visualization of Zoo Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Regras de Associação - Zoo Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Utilizando a livraria \"MLXTEND\"\n",
    "\n",
    "Começamos por importar os dados dos ficheiros e efetuar o merge das tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Getting the data from csv file\n",
    "zoo_file = pd.read_csv('./datasets/zoo.csv')\n",
    "zooClass_file = pd.read_csv('./datasets/zooClass.csv')\n",
    "zoo = pd.DataFrame(zoo_file)\n",
    "zooClass = pd.DataFrame(zooClass_file)\n",
    "\n",
    "zoo_merge = pd.merge(zoo_file, zooClass_file, how='left', left_on='class_type', right_on='Class_Number')\n",
    "zoo_merge.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela Pivot\n",
    "\n",
    "É gerada a tabela Pivot através dos dados do dataset, esta tabela pivot servirá para efetuar o algoritmo das Regras de Aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group\n",
    "ds_grouped = zoo_merge.groupby(['legs', 'class_type'], as_index=False).agg({'Class_Type':'count'})\n",
    "ds_grouped.head(100)\n",
    "\n",
    "# Creating apriori data structure\n",
    "ds_pivot = pd.pivot(data=ds_grouped, index='legs', columns='class_type',\n",
    "                                    values='Class_Type').fillna(0).applymap(lambda x: True if x > 0 else False)\n",
    "\n",
    "ds_pivot.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regras de Associação\n",
    "\n",
    "Através do algoritmo Apriori é possivel gerar as regras de associação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules\n",
    "min_support=0.13\n",
    "freq_itemsets = apriori(ds_pivot, min_support=min_support, use_colnames=True)\n",
    "rules = association_rules(freq_itemsets, metric=\"support\", min_threshold = min_support)    \n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Utilidade \n",
    "\n",
    "Devido ao conteúdo limitado do dataset Zoo Animals, não consguimos desenvolver funções utilizando as Regras de Associação, assim só nos foi possível criar duas funções simples de forma a obter o nome e a classe dos animais.\n",
    "\n",
    "Para tal, utilizamos outro dataset para efetuar esta parte do trabalho com maior profundidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the class type by using animal name\n",
    "def get_animal_type(zoo_info, animal_name):\n",
    "    animal_names_list = zoo_info[\"Animal_Names\"].str.split(', ')\n",
    "    condition = animal_names_list.apply(lambda x: animal_name in x)\n",
    "    \n",
    "    if any(condition):\n",
    "        class_type = zoo_info.loc[condition, \"Class_Type\"].values[0]\n",
    "        return class_type\n",
    "    else:\n",
    "        return 'Animal doesnt exist' \n",
    "    \n",
    "# Function for getting all the animals by using class type.\n",
    "def get_animal_by_classType(zoo_info, classType):\n",
    "    condition = zoo_info[\"class_type\"] == classType\n",
    "    \n",
    "    if condition.any():\n",
    "        animal_names_list = zoo_info.loc[condition, \"Animal_Names\"].values[0].split(', ')\n",
    "        return animal_names_list\n",
    "    else:\n",
    "        return 'Animal class not found' \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(get_animal_by_classType(zoo_merge, 5))\n",
    "get_animal_type(zoo_merge, 'girl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regras de Associação - Groceries dataset\n",
    "\n",
    "## Utilizando a livraria \"MLXTEND\"\n",
    "\n",
    "Começamos por importar os dados do ficheiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Get data from csv file\n",
    "groceries = pd.read_csv('./datasets/groceries.csv')\n",
    "\n",
    "groceries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela Pivot\n",
    "\n",
    "É gerada a tabela Pivot através dos dados do dataset, esta tabela pivot servirá para efetuar o algoritmo das Regras de Aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group\n",
    "ds_grouped = groceries.groupby(['Member_number', 'itemDescription'], as_index=False).agg({'Date':'count'})\n",
    "ds_grouped.head(100)\n",
    "\n",
    "# Create apriori data structure\n",
    "ds_pivot = pd.pivot(data=ds_grouped, index='Member_number', columns='itemDescription',\n",
    "                                    values='Date').fillna(0).applymap(lambda x: True if x > 0 else False)\n",
    "\n",
    "ds_pivot.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regras de Associação\n",
    "\n",
    "Através do algoritmo Apriori é possivel gerar as regras de associação.\n",
    "\n",
    "Usamos o suporte minimo de 0.0651 para tornar o resultado mais preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rules\n",
    "min_support=0.0651\n",
    "freq_itemsets = apriori(ds_pivot, min_support=min_support, use_colnames=True)\n",
    "rules = association_rules(freq_itemsets, metric=\"support\", min_threshold = min_support)    \n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Utilidade \n",
    "\n",
    "Aqui desenvolvemos duas funções para obter os resultados das regras de associação:\n",
    "\n",
    "- **recommend_products**: Esta função simula o processo de recomendação de produtos com base nas regras de associação fornecidas, classifica assim as regras com base na métrica \"lift\" em ordem decrescente e utiliza um ciclo para iterar sobre as regras ordenadas, de forma a verificar se o produto alvo está presente nos antecedentes, se estiver, adiciona os consequentes associados a esse antecedente à lista de recomendações. Por fim, remove o próprio produto alvo da lista de recomendações e retorna as recomendações de produtos.\n",
    "\n",
    "- **get_products**: Esta função chama a função recommend_products de forma a obter as recomendações com base nas regras fornecidas e imprime o produto alvo e as recomendações resultantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for simulating the recommendation process\n",
    "def recommend_products(rules_df, itemDescription, rec_count):\n",
    "    sorted_rules = rules_df.sort_values('lift', ascending=False) \n",
    "    recommended_groceries = [] \n",
    "\n",
    "    for i, exam in sorted_rules[\"antecedents\"].items(): \n",
    "        for j in list(exam):  \n",
    "            if j == itemDescription:  \n",
    "                recommended_groceries.append(\n",
    "                    list(sorted_rules.iloc[i][\"consequents\"]))\n",
    "    \n",
    "    recommended_groceries = list({item for item_list in recommended_groceries for item in item_list}) \n",
    "    recommended_groceries.remove(itemDescription)\n",
    "    return recommended_groceries[:rec_count]\n",
    "\n",
    "# Function to get the recommended products related to a specific product\n",
    "def get_products(product, rules, rec_count):\n",
    "    recomended_products = recommend_products(rules, product, rec_count)\n",
    "    print(f'\\nTarget Grocerie: {product} ')\n",
    "    print(f'Recommended Groceries: {recomended_products}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the recommended groceries\n",
    "get_products('whole milk',rules, 100)\n",
    "get_products('yogurt',rules, 100)\n",
    "get_products('soda',rules, 100)\n",
    "get_products('other vegetables',rules, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão\n",
    "\n",
    "Este projeto foi uma mais-valia para o grupo, pois é uma área interessante e desafiadora. Ao longo deste percurso, dedicamos tempo considerável à pesquisa e aplicação dos conhecimentos adquiridos durante as aulas, que nos permitiu alcançar diversos resultados significativos. Através da implementação prática ds algoritmos mencionados no trabalho, fomos capazes de compreender mais profundamente os conceitos teóricos abordados nas aulas da Unidade Curricular de Inteligência Artificial. Este projeto serviu como um catalisador para consolidar a nossa compreensão sobre os fundamentos destes algoritmos e suas aplicações práticas, através de bases de dados e problemas reais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
