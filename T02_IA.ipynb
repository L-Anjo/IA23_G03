{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Methods\n",
    "\n",
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Automatic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "You can get the data from a file or use the [dataset library](https://scikit-learn.org/stable/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from csv file\n",
    "iris_file = pd.read_csv('./datasets/iris.csv')\n",
    "iris = pd.DataFrame(iris_file)\n",
    "\n",
    "# Prepare the data\n",
    "iris_feature_names = iris.columns.values.tolist()[:4]\n",
    "iris_data = iris[iris_feature_names]\n",
    "iris_target = iris[iris.columns.values.tolist()[4]]\n",
    "iris_target_names = list(set(iris_target))\n",
    "\n",
    "print('Features:',iris_feature_names, '   Classes:', iris_target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative dataset source. Import datasets library\n",
    "from sklearn import datasets\n",
    "#\n",
    "# Prepare the data from datasets library\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Prepare the data\n",
    "iris_feature_names = iris.feature_names\n",
    "iris_target_names = iris.target_names\n",
    "iris_data = iris.data\n",
    "iris_target = iris.target\n",
    "\n",
    "print(iris_feature_names, iris_target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### Build the model and evaluate it\n",
    "Decision trees, Naive bayes and KNN\n",
    "\n",
    "Learn more about cross-validation: https://neptune.ai/blog/cross-validation-in-machine-learning-how-to-do-it-right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the classifiers\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Instantiate the classifier. You can try distinct hyperparameters\n",
    "# clf = GaussianNB()  # Naive Bayes\n",
    "# clf = KNeighborsClassifier(n_neighbors=5)\n",
    "clf = DecisionTreeClassifier(random_state=1234)\n",
    "\n",
    "\n",
    "# Data selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_target, test_size=0.33, random_state=27)\n",
    "\n",
    "# Train the model on the training set\n",
    "clf_model = clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new data\n",
    "# X_new = np.array([[5.1, 3.5, 1.4, 0.2]])\n",
    "# prediction = clf_model.predict(X_new)\n",
    "# print(\"Prediction:\", prediction)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "hd_score = clf_model.score(X_test, y_test)\n",
    "print(\"Houldout test accuracy:\", hd_score)\n",
    "\n",
    "# Evaluate the model using cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "acc_score = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "print(\"CV Mean Accuracy: %0.3f (+/- %0.3f)\" % (acc_score.mean(), acc_score.std()) )\n",
    "f1_score = cross_val_score(clf, X_train, y_train, cv=10, scoring='f1_macro')\n",
    "print(\"Mean F1: %0.3f (+/- %0.3f)\" % (np.mean(f1_score), np.std(f1_score)) )\n",
    "\n",
    "# More metrics: Precision Recall scores and Confusion matrix\n",
    "from sklearn import metrics\n",
    "print(\"Precision, Recall, Confusion matrix, in training test\\n\")\n",
    "print(metrics.classification_report(y_test, clf_model.predict(X_test), digits=3))\n",
    "print(metrics.confusion_matrix(y_test, clf_model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Tree with plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have created the Decision Tree model\n",
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(iris_feature_names)\n",
    "print(iris_target_names)\n",
    "\n",
    "fig = plt.figure(figsize=(30,25))\n",
    "_ = tree.plot_tree(clf_model, \n",
    "                   feature_names=iris_feature_names,  \n",
    "                   class_names=iris_target_names,\n",
    "                   filled=True)\n",
    "\n",
    "# To save the figure to the .png file use the code\n",
    "# fig.savefig(\"decistion_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "----\n",
    "## Clustering with K-Means\n",
    "\n",
    "Iris clustering example (source: https://drlee.io/mastering-kmeans-with-scikit-learn-the-iris-dataset-as-a-playground-797c017add7e)\n",
    "\n",
    "Other options: https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets library\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Preliminary data exploration\n",
    "print(iris.data.shape)     # Output: (150, 4)\n",
    "print(iris.feature_names)  # Output: ['sepal length (cm)', ...]\n",
    "print(iris.data[:5])\n",
    "\n",
    "# Preprocessing the data (standardize)\n",
    "scaler = StandardScaler()\n",
    "scaled_iris = scaler.fit_transform(iris.data)\n",
    "print(scaled_iris[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Optimal Number of Clusters with the Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the within-cluster sum of square across different cluster counts\n",
    "inertia = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(scaled_iris)\n",
    "    inertia.append(kmeans.inertia_)# Plot the elbow graph\n",
    "    \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Within-cluster Sum of Square')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the elbow is at three clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(scaled_iris)\n",
    "\n",
    "# Evaluating\n",
    "# print(clusters)\n",
    "\n",
    "# Adjusting clusters\n",
    "adj_clusters = [(x + 2) %3 for x in clusters]\n",
    "\n",
    "print(confusion_matrix(iris.target, adj_clusters))\n",
    "print(classification_report(iris.target, adj_clusters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose two dimensions to plot (e.g., sepal length and width)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scaled_iris[:,0], scaled_iris[:,1], c=adj_clusters, cmap='viridis', marker='o')\n",
    "plt.title('Visualization of clustered data', fontsize=14)\n",
    "plt.xlabel('Scaled Sepal Length', fontsize=12)\n",
    "plt.ylabel('Scaled Sepal Width', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Cluster Labels into the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Iris dataset to a DataFrame for easier manipulation\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "# Add the cluster labels as a new column to the DataFrame\n",
    "iris_df['cluster'] = adj_clusters\n",
    "\n",
    "# Now let's see the first 5 instances of our new dataset\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3D Visualization of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you've already completed clustering with KMeans\n",
    "# clusters = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Create a 3D figure\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot using the first three features of the Iris dataset\n",
    "ax.scatter(scaled_iris[:,0],  # Sepal length\n",
    "           scaled_iris[:,1],  # Sepal width\n",
    "           scaled_iris[:,2],  # Petal length\n",
    "           c=adj_clusters,            # Use cluster labels as color encoding\n",
    "           cmap='viridis', \n",
    "           marker='o')\n",
    "\n",
    "# Set labels according to the features we used\n",
    "ax.set_xlabel('Scaled Sepal Length')\n",
    "ax.set_ylabel('Scaled Sepal Width')\n",
    "ax.set_zlabel('Scaled Petal Length')\n",
    "\n",
    "# Title of the plot\n",
    "ax.set_title('3D visualization of Iris Clusters')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori python libraries\n",
    "\n",
    "**mlxtend**: Implements manny machine learning algorithms and tools, including association rule mining.\n",
    "\n",
    "**apyori**: Provides functions for manipulating transactional data and for generating association rules and evaluating their quality.\n",
    "\n",
    "**PyCaret**: Low-code ML library for automating machine learning workflows. It provides a wrapper on top of mlxtend for easy implementation of the Apriori algorithm. Current version (3.2.0) does not support association rules. Find more in https://pycaret.org/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Using the MLXTEND library\n",
    "Source: https://medium.com/codex/what-is-association-rule-learning-abd4a76144d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Read the file\n",
    "medical_exams = pd.read_excel('./datasets/medical_exams.xlsx',sheet_name='Sheet1')\n",
    "\n",
    "# To remove missing values: raw_data.dropna(inplace=True)\n",
    "# To filter qty > 0:        raw_data = raw_data[raw_data[\"quantity\"] > 0]\n",
    "# To remove end characters: raw_data[\"product_name\"] = raw_data[\"product_name\"].str.replace(r' - .*$', '')\n",
    "medical_exams.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table \n",
    "For using the apriori algorithm we need to pivot the table.\n",
    "\n",
    "If the product is in the invoice, the intersection cell will be “True”. If is not, it will be “False”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group\n",
    "ds_grouped = medical_exams.groupby(['RequestID', 'ExamID'], as_index=False).agg({'ExamName':'count'})\n",
    "ds_grouped.head(100)\n",
    "\n",
    "# Create apriori data structure\n",
    "ds_pivot = pd.pivot(data=ds_grouped, index='RequestID', columns='ExamID',\n",
    "                                    values='ExamName').fillna(0).applymap(lambda x: True if x > 0 else False)\n",
    "ds_pivot.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rules (Association Rule Learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the rules\n",
    "min_support=0.01\n",
    "freq_itemsets = apriori(ds_pivot, min_support=min_support, use_colnames=True)    \n",
    "rules = association_rules(freq_itemsets, metric=\"support\", min_threshold=min_support)    \n",
    "rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the product names by using product id\n",
    "def get_item_name(medical_exams, exams):\n",
    "    if type(exams) != list:\n",
    "        exam_name = medical_exams[medical_exams[\"ExamID\"] == exams][\"ExamName\"].values[0]\n",
    "        return exam_name\n",
    "    else:\n",
    "        exam_names = [medical_exams[medical_exams[\"ExamID\"] == exams][\"ExamName\"].values[0] for exams in exams]\n",
    "        return exam_names\n",
    "\n",
    "# ———————————————————————————————\n",
    "# Function to get the recommended products related to a specific product\n",
    "def get_golden_shot(target_id, dataframe, rules, rec_count):\n",
    "    target_exam = get_item_name(dataframe,target_id)\n",
    "    recomended_exam_ids = recommend_items(rules, target_id, rec_count)\n",
    "    recomended_exam_names = get_item_name(dataframe,recommend_items(rules, target_id, rec_count))\n",
    "    print(f'\\nTarget Exam ID (which is in the cart): {target_id}   Exam Name: {target_exam}')\n",
    "    print(f'Recommended Exams: {recomended_exam_ids}\\nExam Names: {recomended_exam_names}')\n",
    "\n",
    "# ———————————————————————————————\n",
    "# Function for simulating the recommendation process\n",
    "def recommend_items(rules_df, exam_id, rec_count):\n",
    "    sorted_rules = rules_df.sort_values('lift', ascending=False) \n",
    "    # we are sorting the rules dataframe by using \"lift\" metric\n",
    "    recommended_exams = [] \n",
    "\n",
    "    for i, exam in sorted_rules[\"antecedents\"].items(): \n",
    "        for j in list(exam):  \n",
    "            if j == exam_id:  \n",
    "                recommended_exams.append(\n",
    "                    list(sorted_rules.iloc[i][\"consequents\"]))\n",
    "      \n",
    "    recommended_exams = list({item for item_list in recommended_exams for item in item_list}) \n",
    "    recommended_exams.remove(exam_id)\n",
    "    return recommended_exams[:rec_count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the recommended exams\n",
    "get_golden_shot(7, medical_exams,rules, 10)\n",
    "get_golden_shot(64, medical_exams,rules, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "607079afe3a2a2d35ed9965cd3a5fc45cd7e1ba906577a56395d9b6815374bc5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
